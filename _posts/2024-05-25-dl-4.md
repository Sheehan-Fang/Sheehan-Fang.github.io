## 注意力机制

注意力机制是一种包含自主性提示的神经网络模型。

### 一、注意力机制

#### 1. Q（查询）K（键）V（值）

![](https://sheehan-fang.github.io/images/picture/attention/1.png)

​        注意力机制可以类似于人眼的检索。比如当你面前摆放有一系列的物品，这些物品就相当于注意力机制中的值。人眼看到物体后，在大脑中呈现的物品内容相当于键。当人需要拿取某个物品时，拿取物品的指令相当于查询。通过拿取物品的指令和大脑中呈现的物品进行相互作用，注意力便会落在某个物品上，这个过程和注意力机制筛选最终输出的过程非常相似。

​        注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。查询（自主性提示）与键（非自主性提示）之间的匹配，这样就能找到最匹配的值（感官输入）。

#### 2.**注意力评级函数**

![](https://sheehan-fang.github.io/images/picture/attention/2.png)

##### **（1）计算公式**

假设有查询 $q \in \mathbb{R}^d$ 和m个键值对$(k_1,v_1),...,(k_m,v_m)$,其中$k_i \in \mathbb{R}^k,v_i \in \mathbb{R}^v$。那么注意力机制的计算函数f可以表示为加权和：
$$
f(q, (k_1, v_1), \dots, (k_m, v_m)) = \sum_{i=1}^{m} softmax(a(q, k_i)) v_i = \sum_{i=1}^{m} \frac{exp(a(q,k_i))}{\sum_{j=1}^{m}exp(a(q,k_j))}v \in \mathbb{R}^v
$$
其中$a(,)$是注意力的计算函数。

##### （2）加性注意力机制

加性注意力机制通常使用在：**查询和键是不同长度的矢量**。

加性注意力机制的评分函数为：
$$
a(q,k)=\omega_v^Ttanh(W_q+W_kk) \in \mathbb{R}
$$
其中可学习的参数是$W_q \in \mathbb{R}$、$W_k \in \mathbb{R}^{h×k}$和$w_v \in \mathbb{R}^h$。将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数 h hh。通过使用tanh作为激活函数，并且禁用偏置项。

**（3）缩放点积注意力机制**

加性注意力机制通常使用在：**查询和键是相同长度的矢量**。

对与查询矩阵$Q \in \mathbb{R}^{n×d}$、键矩阵$K \in \mathbb{R}^{m×d}$和值矩阵$V \in \mathbb{R}^{m×v}$的缩放点积注意力是：
$$
a(q,k)=softmax(\frac{QK^T}{\sqrt{d}})V \in \mathbb{R}^{n×v}
$$

### 二、自注意力机制

自注意力机制是注意力机制的一个特例，输入序列中的每个位置既作为查询 q，又作为键 k 和值 v。换句话说，序列中的每个元素对自身及其他元素的重要性都要被评估。自注意力机制的公式和缩放点积注意力机制相似，公式为：
$$
a(q,k)=softmax(\frac{QK^T}{\sqrt{d_k}})V \in \mathbb{R}^{n×v}
$$
其中Q 是查询矩阵（来自输入序列），K 是键矩阵（来自输入序列），V 是值矩阵（来自输入序列），$d_k$是键向量的维度，用于缩放。

### **三、多头注意力机制**

多头注意力机制是在自注意力机制的基础上扩展而来的，可以理解为**由多个注意力头组合而成的注意力集群**。核心思想是：通过多个独立的注意力头 ，可以让模型从不同的“角度”去捕捉输入序列中各部分的相关性。

每个注意力头有自己独立的查询、键和值线性变换，这些变换后的矩阵通过自注意力机制得到不同的输出，然后将它们拼接起来，最后再进行一次线性变换。

多头注意力的公式如下：
$$
MultHead(Q,K,V)=Concat(head_1,...,head_h)W^O
$$
其中每一个head都表示一个注意力头，注意力头可以是上述任何一种注意力机制。
