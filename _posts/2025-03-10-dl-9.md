---
title: ' TokenSkip: 在大型语言模型中的可控链式思维压缩'
date: 2025-3-10
permalink: /posts/2025/03/dl-9/
tags:
  - 深度学习
  - 思维链
---

事实证明，思维链（CoT）可以有效增强大型语言模型（LLM）的推理能力。OpenAI 的 o1 和 DeepSeek-R1 等最新进展表明，在推理过程中增加 CoT 序列的长度可以进一步提高 LLM 的推理性能。然而，由于 LLM 解码的自回归特性，较长的 CoT 输出会导致推理延迟的线性增加，从而对用户体验产生不利影响，尤其是当 CoT 超过 10,000 个 token 时。为了解决这一局限性，我们分析了 CoT 输出中标记的语义重要性，发现它们对推理的贡献各不相同。基于这一洞察力，我们提出了一种简单而有效的方法--TokenSkip，它能让 LLM 选择性地跳过不太重要的标记，从而实现可控的 CoT 压缩。在各种模型和任务中进行的大量实验证明，TokenSkip 能有效减少 CoT 标记的使用，同时保持强大的推理性能。值得注意的是，当应用于 Qwen2.5-14B-Instruct 时，TokenSkip 在 GSM8K 上减少了 40% 的推理标记（从 313 个减少到 181 个），而性能下降不到 0.4%。

## 一、背景

1. ### Token重要性

   <div align=center><img src="https://sheehan-fang.github.io/images/picture/TokenSkip/1.png"/></div>

   Token冗余一直被认为是 LLM 效率中一个长期存在的基本问题。为解决这一问题，可以根据 LLM 的语义置信度来衡量文本中标记的重要性，公式如下：

   $$
   I_1(x_i) = - \log P (x_i | x_{<i}; \theta_{M_L})
   $$

   其中，$\mathbf{x}=\{x_i\}_{i = 1}^n$ 是给定的文本， $x_i$ 代表Token， $M_L$ 代表用于计算置信度的大模型。

   但是这种方法存在一定的弊端：

   - 大语言模型困惑度的内在特性导致句末词元的重要性度量较低（即置信度较高）。这种位置依赖性影响了每个词元的事实重要性测量。
   - 因果语言模型中的单向注意力机制可能无法捕捉文本中词元重要性所需的所有关键信息。

   为了解决这些局限性，LLMLingua-2引入了利用类似于 BERT 的双向语言模型来测量标记的重要性。它利用 GPT-4将每个标记标记为 "重要 "或 "不重要"，并以标记分类为目标训练双向 LM。这样，标记的重要性由每个标记的预测概率来衡量：

   $$
   I_2(\xi_i) = P(\xi_i | x\leq n;\theta_{M_B})
   $$

   其中$M_B$代表双向语言模型。

2. ### 思维链恢复

   <div align=center><img src="https://sheehan-fang.github.io/images/picture/TokenSkip/2.png"/></div>

​	被裁剪的思维链仍然可以被恢复。

## 二、方法

1. ### Token剪枝

   给定一个目标大语言模型 $M$ ，假设他的思维链轨迹是 $c=\{c_i \}_{i=1}^m$ ，目标压缩比为 $\gamma \in [0, 1]$ 。TokenSkip首先计算每个链式思维词元 $I(c)$ 的语义重要性，然后，根据重要性值对这些词元进行降序排序。接下来，计算这些重要性值的 $\gamma $ 百分位数，表示剪枝的阈值：

   $$
   I_{\gamma} = np.percentile([I(c_1), \ldots, I(c_m)], \gamma)
   $$

   最后，重要性值大于或等于 $I_{\gamma}$ 的思维链Token被保留在压缩的思维链轨迹中。

   $$
   E_c = \{c_i \mid I(c_i) \geq I_{\gamma} \}, \quad 1 \leq i \leq m.
   $$
   
   其中$E_c$表示被压缩之后的思维链。

2. ### 训练

   给定一个包含N个样本的训练数据集D和一个目标大语言模型M，我们首先使用M获得N个CoT轨迹。然后，我们筛选出答案不正确的轨迹，以确保训练数据的高质量。对于剩余的CoT轨迹，我们以随机选择的压缩比γ对每个CoT进行修剪。

   对于每个⟨问题，压缩的链式推理，答案⟩，我们在问题后插入了压缩比 $γ$ 。最后，每个训练样本的格式如下：

   $$
   Q\quad[EOS]\quadγ\quad[EOS] \quad Compressed\;CoT \quad A
   $$

   其中*[EOS]*代表结束符号，*Compressed CoT A*代表压缩后的链式思维以及答案A。

   正式地，给定一个问题 $x$ 、压缩比 $\gamma$ ，以及输出序列 $y = \left \{ y_i \right \}_{i=1}^l$ ，其中包括压缩思维链 $\tilde{c_1}$ 和答案 $a$ ，损失函数为

   $$
   L = \sum_{i=1}^{l} \log P(y_i | x, \gamma, y_{<i}; \theta_M)
   $$

   其中 $y = \{\tilde{c_1}, \cdots, \tilde{c_{m'}}, a_1, \cdots, a_t\}$，且保持答案 $$a = \{ a_i \}_{i=1}^t$$ 不变。

   为了保持大型语言模型的推理能力，我们还在训练数据中包含了一部分原始 CoT 轨迹，γ 设置为 1。

3. ### 推断

   <div align=center><img src="https://sheehan-fang.github.io/images/picture/TokenSkip/3.png"/></div>

   给定一个问题 $x$ 和压缩比 $\gamma$ ，TokenSkip 的输入提示遵循在微调中采用的相同格式，即 $Q\;[EOS]\;γ\;[EOS]$。大语言模型 $M$ 依次预测输出序列 $\hat{y}$ ：

   $$
   \hat{y} = \arg \max_{y^*} \sum_{l'} \log P(y_j | x, \gamma, y_{<j}; \theta_M)
   $$

   其中 $y=\{\hat{c_1}, \cdots, \hat{c_{m''}}, a_1, \cdots, a_t\}$ 表示输出序列，包括思维链Token的 $\hat{c}$ 和答案 $\hat{a}$。

