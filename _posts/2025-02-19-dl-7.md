---
title: 'deepseek技术报告解读'
date: 2025-2-19
permalink: /posts/2025/02/dl-7/
tags:
  - 深度学习
  - 强化学习
  - 知识蒸馏
---
# deepseek技术报告解读 #

## 一、文章结论 ##

1.  即使不使用监督微调（SFT）作为冷启动，也能通过大规模强化学习（RL）显著提高推理能力。
2. 大型模型的推理模式可以被提炼到小型模型中，从而比通过小型模型上的 RL 发现的推理模式具有更好的性能。
3. DeepSeek-R1-Zero 不需要任何有监督的微调数据，就能获得强大的推理能力。此外，还可以通过应用多数表决进一步提高。

## 二、DeepSeek-R1-Zero ##

### 1、Group Relative Policy Optimization（GRPO）	 ###

- **组相对优势估计（Group Relative Advantage Estimation）**

GRPO 不是直接计算单个样本的优势函数 $A(s,a)$，而是通过将样本划分为多个组，每个组中的样本共享一定的统计信息，以减少方差。对于某个状态 $s$，它的优势估计计算方式如下：
$$
A^{\text{group}}(s, a) = A(s, a) - \mathbb{E}_{a' \sim \pi_{\text{old}}} [A(s, a')]
$$
其中，$\mathbb{E}_{a' \sim \pi_{\text{old}}} [A(s, a')]$表示对旧策略下所有可能动作的优势函数取均值，确保更新时减少策略估计的偏差。

- **相对策略更新（Relative Policy Update）**

传统 PPO 通过限制策略更新的 KL 散度来保证更新的稳定性，而 GRPO 通过引入组相对性，约束策略更新使其朝向“相对最优”的方向收敛，而不是直接进行大步更新。GRPO 通过以下优化目标进行更新：
$$
\max_{\theta} \mathbb{E}_{(s, a) \sim \pi_{\text{old}}} 
\left[ 
\min\left( r_{\theta} A^{\text{group}}, \text{clip}(r_{\theta}, 1-\epsilon, 1+\epsilon) A^{\text{group}} \right) 
\right]
$$
其中，$r_\theta = \frac{\pi_\theta(a|s)}{\pi_{\text{old}}(a|s)}$是策略更新比率，$\epsilon$ 是裁剪系数。相比 PPO，GRPO 在裁剪时考虑了组相对优势，使得更新更加稳定。

### 2、奖励模型 ###

奖励是训练目标的来源，它决定了 RL 的优化方向。DeepSeek-R1-Zero的训练采用了基于规则的奖励系统，主要包括两类奖励：

- 准确性奖励：依据模型的正确性进行评估。
- 格式奖励：强制模型将其思考过程置于"<think>"和"</think>"标记之间。

### 3、训练模板 ###

```html
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:
```

### 4、创新点 ###

- **反思能力**：AI 在推理过程中发现错误时，会自动调整自己的思路，甚至会标记“这一步可能有问题”，然后重新思考解法。
- **自我验证：AI 会使用不同的方法来检查自己的答案是否正确。
- **生成更长的推理链**：起初 AI 只会做 2-3 步推理，后来能逐步扩展到 10 步以上，从简单问题到复杂问题的推理能力都在增强。

### 5、缺点 ###

- **语言混乱**：由于没有SFT过程，导致可能会出现多种语言混杂的问题，影响可读性。
- **可读性差**：可能会生成大量重复内容，甚至逻辑混乱，导致输出质量下降。
- **训练难度大**：由于没有 SFT 作为起点，导致强化学习的收敛速度变慢。

## 三、DeepSeek-R1 ##

### 1、冷启动 ###

为了防止基模型在强化学习训练早期出现不稳定的冷启动阶段，DeepSeek-R1使用数千个长链推理（CoT）数据作为冷启动数据。

冷启动数据来源如下：

- 使用ChatGPT-4 或 DeepSeek-V3生成详细的数学推理步骤，并筛选其中质量较高的部分。
- 由于 DeepSeek-R1-Zero 具备一定的推理能力，研究人员从中挑选出可读性较好的推理结果，并重新整理后作为冷启动数据。
- DeepSeek-R1在后续测试中生成的优质数据。

### **2、以推理为导向的强化学习** ###

为增强模型可读性，DeepSeek-R1使用了一种可读的模式，在每个回复的末尾包含摘要，并过滤掉不适合阅读的回复。输出格式定义为 

```html
|special_token|
<reasoning_process>
|special_token|
<summary>
```

例如要求模型进行深度思考时，会设置格式为

```html
<think>
	推理内容
</think>
最终输出的内容
```

其中推理过程是针对查询的链式推理，而摘要用于总结推理结果。

在训练过程中，我们发现 CoT 经常出现语言混杂的问题，尤其是当 RL 提示涉及多种语言时。为了缓解语言混合的问题，我们在 RL 训练过程中引入了语言一致性奖励，其计算方法是 CoT 中目标语言单词所占的比例。尽管消融实验表明，这种一致性会导致模型性能略有下降，但这种奖励符合人类的偏好，使其更具可读性。最后，我们将推理任务的准确性和语言一致性奖励直接相加，形成最终奖励。然后，我们对微调后的模型进行 RL 训练，直到它在推理任务上达到收敛为止。

### 3、拒绝采样（Rejection Sampling, RS） ###

- **什么是拒绝采样**
  - AI 生成的答案并不总是正确的，有时候它会输出胡言乱语、逻辑错误或者无意义的推理链。如果不进行筛选，这些错误答案可能会影响模型的学习过程，甚至让 AI 形成错误的推理模式。
  - 为了解决这个问题，DeepSeek-R1采用了一种称为拒绝采样的方法，让 AI 在训练过程中只保留最优质的推理答案，从而提升整体推理能力。
- **拒绝采样的工作原理**
  - 在同一个问题上生成多个不同的答案。
  - 计算每个答案的质量分数。
  - 只保留质量最高的答案用于后续训练，丢弃较差的答案。


### 4、知识蒸馏 ###

- **大模型生成高质量数据**：让DeepSeek-R1生成大量的推理过程，这些答案不仅包含最终结果，还包含完整的推理链条，帮助小模型理解解题逻辑。
- **小模型学习大模型的输出**：小模型基于大模型的输出进行监督微调（SFT），来模仿大模型的推理过程。
- **优化训练策略**：基于特定任务的内容进行优化策略设计。



