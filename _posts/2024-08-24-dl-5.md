---
title: 'Transformer详解'
date: 2024-08-24
permalink: /posts/2024/08/dl-5/
tags:
  - 深度学习
  - Transformer
  - 模型架构
---

Transformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构，它在2017年由Vaswani等人首次提出。Transformer架构引入了自注意力机制（self-attention mechanism），这是一个关键的创新，使其在处理序列数据时表现出色。

Transformer可以由输入、编码器、解码器和输出四个部分组成。

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/1.png"/></div>



## 一、输入

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/8.png"/></div>

Transformer的输入包含Input输入和Output输入。以机器翻译为例，在训练阶段，Input是原始文本，Output是对应翻译后的标准答案；在推理阶段，Input是待翻译的文本，Output是翻译的结果。

如下面的例子所示：

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/2.png"/></div>

在训练阶段，原始文本<bos> I love you <eos> <pad> <pad> 和标准答案 <bos> 我 爱 你 <eos> <pad>会同时送入Transformer进行运算，但是对于标准答案会Mask当前不该出现的内容。比如训练时输入的Output依次是：

- \<bos\>
- \<bos\> 我
- \<bos\> 我 爱 
- \<bos\> 我 爱 你
- \<bos\> 我 爱 你 \<eos\>

Output中的\<pad\>不会被输入，这是因为我们在最后一次推理时，只会传入前n-1个token。

在推理阶段，待翻译的\<bos\> I love you \<eos\> \<pad\> \<pad\> 会直接送入编码器，而Output部分的输入会是前几轮输出的叠加，其效果和上面展示的训练阶段的Output输入方式类似。



## 二、编码器

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/3.png"/></div>

编码器部分主要由自注意力层、残差层和全连接层组成。



### 1、注意力层

#### 自注意力机制

自注意力层可以理解为Self-Attention接受一个Sequence（一排向量，可以是输入，也可以是前面隐层的输出），然后Self-Attention输出一个长度相同的Sequence，该Sequence的每个向量都充分考虑了上下文。因此，我们需要计算每一个向量和其他向量之间的相关性。

对于相关性，我们首先想到的是使用余弦相似度，但是这样做并不妥当。例如，句子 *I saw a saw* 的 *saw* 和 *saw* 相关性一定很高(两个一样的向量夹角为0)，因此模型在处理这个单词的时候会把注意力集中在另外一个相同的单词上，这显然不合理。因此，我们需要进行线性变换来解决这个问题。

我们使用矩阵Q对当前向量进行线性变换，矩阵K对其他向量进行线性变换，则可以得到相似度α。但是这样求出来的所有α相加并不为1，所以需要对所有的α进行softmax处理。之后再和使用矩阵V进行线性变换的结果进行相乘相加。具体步骤如下：


$$
\begin{align*}
& 1、 求出查询向量q^i,公式为：q^i=W^q·a^i \\
& 2、 求出比较向量k^i,公式为：k^j=W^k·a^j \\
& 3、 求出相似度\alpha_{i,j},公式为：\alpha_{i,j}=q^i·k^j \\
& 4、 对相似度进行归一化，得到归一化结果\alpha^`_{i,j},公式为：\alpha^`_{i,j}=\frac{e^{\alpha_{i,j}}}{\sum^n_{t=0}e^{\alpha_{i,t}}} \\
& 5、 求出注意力向量v^j,公式为：v^j=W^v·a^j \\
& 6、 求出结果b^i,公式为：b^i=\sum^n_j\alpha^`_{i,j}·v^j
\end{align*}
$$


显然，这种结构不利于并行化，所以我们需要把这种结构改为矩阵的形式。


$$
Q_{d_k \times n}=(q^1,q^2,...,q^n)\\
K_{d_k \times n}=(k^1,k^2,...,k^n)\\
V_{d_v \times n}=(v^1,v^2,...,v^n)
$$


矩阵化后的注意力机制可表示为：


$$
Attention(Q,K,V)=softmax(\frac{Q·K^T}{\sqrt{d_k}})V\\
\begin{align*}
&其中，d_k是Q矩阵和K矩阵的行维度，也就是上面的Q_{d_k \times d}中的d_k。\\
&矩阵相乘会放大原有矩阵的标准差，放大的倍数约为\sqrt{d_k}。为了将标准差缩放回原来的大小，所以要除以\sqrt{d_k}。
\end{align*}
$$



#### 多头注意力

注意力机制可以表示为下面的图：

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/4.png"/></div>

而MultiHead Attention在带入公式前做了一件事情，就是拆，它按照“词向量维度”这个方向，将Q,K,V拆成了多个头。这里我的head数为4。既然拆成了多个head，那么之后的计算，也是各自的head进行计算，如图所示：

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/5.png"/></div>

但这样拆开来计算的Attention使用Concat进行合并效果并不太好，所以最后需要再采用一个额外的矩阵，对Attention再进行一次线性变换，如图所示：

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/6.png"/></div>

到这里也能看出来，**head数并不是越多越好**。而为什么要用MultiHead Attention，Transformer给出的解释为：**Multi-head attention允许模型共同关注来自不同位置的不同表示子空间的信息**。



### 2、残差层

在Encoder中残差层被使用了两次。一次是对多头注意力输出的结果进行处理，另一次是对全连接的结果进行处理。两次处理的公式如下：


$$
Output=LayerNorm(X+MultiHeadAttention(X))\\
Output=LayerNorm(X+FeedForward(X))
$$


<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/7.png"/></div>

因此，**残差层可以理解成为往结果里添加了一个残差块并进行归一化**。加入残差块的目的是为了防止在深度神经网络的训练过程中发生退化的问题，退化的意思就是深度神经网络通过增加网络的层数，Loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，Loss反而增大。归一化目的是加快训练速度、提高训练的稳定性。



### 3、全连接层

全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换。这两层网络就是为了将输入的Z映射到更加高维的空间中然后通过非线性函数ReLU进行筛选，筛选完后再变回原来的维度。全连接层的公式可以表示为：


$$
FFN(x)=max(0,W_1·x+b)·W_2+b_2
$$



## 三、解码器

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/9.png"/></div>

解码器和编码器的结构几乎一致。相比之下多了一个Masked Multi-Head Attention。

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/10.png"/></div>

通过上面的流程图我们可以发现，解码器当前轮次的解码需要依赖之前轮次的解码结果。但是多头注意力机制会对之前已完成的编码产生影响，因此需要遮蔽一部分内容使得每次只对当前轮次的数据进行操作。具体的mask操作是：**在求Q和K的相关性时，由于Q与K相等，前面的query不应该和后面的key产生相关性，需要把当前query对于它之后的key的相关性置为0，因此就需要mask掉，最后产生一个类似下三角矩阵的结果**。下面是一个例子：

第一次,我们只有一个输入变量$v_1$,所以是:


$$
[o_1]=softmax(\frac{[q_1]·[k_1]^T}{\sqrt{d_k}})·[v_1]
$$


第二次,我们有两个输入变量$v_1$和$v_2$。若不使用mask,则结果为：


$$
\begin{bmatrix} o_1\\o_2 \end{bmatrix} = 
softmax(\frac{\begin{bmatrix} q_1\\q_2 \end{bmatrix}·\begin{bmatrix} k_1\\k_2 \end{bmatrix}^T}{\sqrt{d_k}})·\begin{bmatrix} v_1\\v_2 \end{bmatrix}=
softmax(\frac{\begin{bmatrix} q_1·k_1 & q_1·k_2\\q_2·k_1 & q_2·k_2 \end{bmatrix}}{\sqrt{d_k}})·\begin{bmatrix} v_1\\v_2 \end{bmatrix}
$$


因此，我们可以发现，在第一轮结束后，$o_1=softmax(\frac{q_1·k_1}{\sqrt{d_k}})·v_1$。

但在第二轮结束后，$o_1=softmax(\frac{q_1·k_1}{\sqrt{d_k}})·v_1+softmax(\frac{q_1·k_2}{\sqrt{d_k}})·v_2$。

所以通过采用对关系矩阵进行mask的操作，来保证一致性,可以避免计算本轮参数时对之前参数产生影响。例如：


$$
\begin{bmatrix} o_1\\o_2 \end{bmatrix} = 
softmax(\frac{\begin{bmatrix} q_1·k_1 & 0\\q_2·k_1 & q_2·k_2 \end{bmatrix}}{\sqrt{d_k}})·\begin{bmatrix} v_1\\v_2 \end{bmatrix}
$$


依次类推，如果我们执行到第$n$次时，就因该会变为：


$$
\begin{bmatrix} o_1\\ \vdots \\o_n \end{bmatrix}=softmax(\frac{A}{\sqrt{d_k}})·\begin{bmatrix} v_1\\ \vdots \\v_n \end{bmatrix}\ \ \ \ \ \
其中，A=\begin{bmatrix}
q_1·k_1 &    0    & \cdots & 0 & 0\\
q_1·k_2 & q_2·k_2 & \cdots &     0       &    0   \\
\vdots  &  \vdots & \ddots & \vdots      & \vdots \\
q_1·k_{n-1} & q_2·k_{n-1} & \cdots & q_{n-1}·k_{n-1} & 0  \\
q_1·k_n & q_2·k_n & \cdots & q_{n-1}·k_n & q_n·k_n 
\end{bmatrix}
$$


**使用mask时，我们可以保证前面的词不会具备后面词的信息，这样就可以保证Transformer的输出不会因为传入词的多少而改变。**



## 四、输出

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/11.png"/></div>

Output如图中所示，首先经过一次线性变换（线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的，被称为对数几率的向量里），然后Softmax得到输出的概率分布（softmax层会把向量变成概率），然后通过词典，输出概率最大的对应的单词作为我们的预测输出。



## 五、小结

1. 优点：
   - 效果好
   - 可以并行训练，速度快
   - 很好的解决了长距离依赖的问题
2. 缺点：完全基于self-attention，对于词语位置之间的信息有一定的丢失，虽然加入了positional encoding来解决这个问题，但也还存在着可以优化的地方。
3. 总览：Transformer的全流程动态图如下：

<div align=center><img src="https://sheehan-fang.github.io/images/picture/Transformer/1.gif"/></div>
