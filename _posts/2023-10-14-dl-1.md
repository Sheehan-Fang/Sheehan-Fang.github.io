---
title: '卷积神经网络'
date: 2023-10-14
permalink: /posts/2023/10/dl-1/
tags:
  - 深度学习
  - 卷积网络
  - 模型架构
---
卷积神经网络详解

### 一、卷积神经网络的结构

![](..\images\picture\CNN\1.png)

- 输入层（Input layer）
- 卷积计算层（CONV layer）
- ReLU激励层（ReLU layer）
- 池化层（Pooling layer）
- 全连接层（FC layer）

### 二、输入层（数据预处理）

输入层是**对原始数据进行预处理**

- **去均值**：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。
- **归一化**：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰。
- **PCA/白化**：用PCA降维；白化是对数据各个特征轴上的幅度归一化。

### 三、卷积计算层（捕捉数据特征）

1. **过滤器**：过滤器在输入图像上滑动，通过卷积操作生成特征图。

2. **感受野**：感受野是指神经元能够感知到的输入区域。对于一个特定的神经元，其感受野是输入图像中的一个子区域。感受野的大小影响神经元能捕捉到的图像信息的范围。

3. **卷积**：将过滤器应用于输入图像的一个小区域，计算它们的点积和，然后滑动过滤器到下一个位置，重复这个过程。卷积操作可以看作是对输入图像进行的特征提取过程，过滤器学习到的特征可以是边缘、纹理、形状等。

4. **深度**：指图像第三个维度的大小（如RGB图像的深度为3）。

5. **步幅**：指窗口一次滑动的长度。

6. **填充值**：连续使用卷积会导致数据尺寸大幅度缩小，但在网络的早期层中，我们想要尽可能多地保留原始输入内容的信息，这样我们就能提取出那些低层的特征。因此在数据外围进行填充可以使卷积前后的数据尺寸不变。（即用0来填补卷积后减少的尺寸）。

7. **卷积层的计算过程**：

   ![](..\images\picture\CNN\1.webp)

### 四、激活层（拟合非线性函数）

- CNN采用的激活函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱。
- **激励层的实践经验**：
  - 不要用sigmoid！不要用sigmoid！不要用sigmoid！
  - 首先试RELU，因为快，但要小心点
  - 如果RELU失效，请用Leaky ReLU或者Maxout
  - 某些情况下tanh倒是有不错的结果，但是很少

### 五、池化层(减小过拟合)

- 池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。

![](..\images\picture\CNN\2.png)

- **Max pooling**：设定基础矩形框后，在框里取最大值作为压缩后的值。（常用）
- **average pooling**：设定基础矩形框后，在框里取平均作为压缩后的值。

![](..\images\picture\CNN\2.webp)

### 六、全连接层

两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。(跟传统的神经网络神经元的连接方式一样)

![](..\images\picture\CNN\3.png)

### 七、CNN小结

1. 训练方法：
   - 同一般机器学习算法，先定义Loss function，衡量和实际结果之间差距。
   - 找到最小化损失函数的W和b， CNN中用的算法是SGD（随机梯度下降）。
2. 优缺点
   - 优点：共享卷积核，对高维数据处理无压力。无需手动选取特征，训练好权重，即得特征分类效果好。
   - 缺点：需要调参，需要大样本量，训练最好要GPU。物理含义不明确（不知道每个卷积层到底提取到的是什么特征）。
3. [典型CNN](https://cloud.tencent.com/developer/article/1481016)
