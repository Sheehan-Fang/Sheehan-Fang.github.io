---
title: ' Transformer库基础知识'
date: 2025-3-27
permalink: /posts/2025/03/tool-1/
tags:
  - 技术工具
  - Transformer包
  - 大模型部署
---

Transformer是一个流行的深度学习库，用于处理序列到序列的任务，例如机器翻译和自然语言生成。它是由Google的研究员于2017年提出的，自此已经成为了许多自然语言处理领域的标准模型。
Transformer中最著名的模型是Transformer Encoder-Decoder架构，其中编码器将输入序列转换为连续的表示形式，解码器将该表示形式转换为输出序列。该模型的核心是self-attention机制，使得模型能够同时考虑输入序列中的所有位置。
除了自注意力机制，Transformer还包括多头注意力机制和残差连接等重要组件。它使用标准的反向传播算法进行训练，并且通常使用随机梯度

## 一、Transformer基础知识

1. ### token、tokenization 和 tokenizer

   - `token`: 可以理解为最小语义单元，翻译的话可以是词元、令牌、词，也可以是 word/char/subword，单理解就是单词和标点
   - `tokenization`: 是指**分词**过程，目的是将输入序列划分成一个个词元（`token`），保证各个词元拥有相对完整和独立的语义，以供后续任务（比如学习 embedding 或作为 LLM 的输入）使用
   - `Tokenizer`: 在 transformers 库中，`tokenizer` 就是实现 `tokenization` 的对象，每个 tokenizer 会有不同的 vocabulary。在代码中，tokenizer 用以将输入文本序列划分成 tokenizer vocabulary 中可用的 `tokens`

2. ### input IDs

   `LLM` 唯一必须的输入是 `input ids`，本质是 `tokens` 索引，即数整数向量。

   - 将输入文本序列转换成 tokens，即 tokenized 过程

   - 将输入文本序列转换成 input ids，即输入编码过程，数值对应的是 tokenizer 词汇表中的索引

     ```python
     from transformers import BertTokenizer
     
     sequence = "A Titan RTX has 24GB of VRAM"
     tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased") 
     
     tokenized_sequence = tokenizer.tokenize(sequence) # 将输入序列转换成tokens，tokenized 过程
     inputs = tokenizer(sequence) # 将输入序列转化成符合模型输入要求的 input_ids，编码过程
     encoded_sequence = inputs["input_ids"]
     
     print(tokenized_sequence)
     print(encoded_sequence)
     print("[INFO]: length of tokenized_sequence and encoded_sequence:", len(tokenized_sequence), len(encoded_sequence))
     
     """
     ['A', 'Titan', 'RT', '##X', 'has', '24', '##GB', 'of', 'VR', '##AM']
     [101, 138, 28318, 56898, 12674, 10393, 10233, 32469, 10108, 74727, 36535, 102]
     [INFO]: length of tokenized_sequence and encoded_sequence: 10 12
     """
     ```

     值得注意的是，**调用 `tokenizer()` 函数返回的是字典对象**，包含相应模型正常工作所需的所有参数，tokens 索引在键 `input_ids` 对应的键值中。同时，**tokenizer 会自动填充 "special tokens"**（如果相关模型依赖它们），这也是 tokenized_sequence 和 encoded_sequence 列表中长度不一致的原因。

     ```python
     decoded_sequence = tokenizer.decode(encoded_sequence)
     print(decoded_sequence)
     """
     [CLS] A Titan RTX has 24GB of VRAM [SEP]
     """
     ```

3. ###  attention mask

   注意掩码（`attention mask`）是一个可选参数，一般在将输入序列进行**批处理**时使用。作用是告诉我们哪些 `tokens` 应该被关注，哪些不用。因为如果输入的序列是一个列表，每个序列长度是不一样的，通常是通过填充的方式把他们处理成同一长度。原始 token id 是我们需要关注的，填充的 id 是不用关注的。

   attention mask 是二进制张量类型，值为 `1` 的位置索引对应的原始 `token` 表示应该注意的值，而 `0` 表示填充值。

   ```python
   from transformers import AutoTokenizer
   
   sentence_list = ["We are very happy to show you the   Transformers library.",
               "Deepspeed is faster"]
   model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
   tokenizer = AutoTokenizer.from_pretrained(model_name)
   
   padded_sequences = tokenizer(sentence_list, padding=True, return_tensors="pt") # 字典类型
   
   print(padded_sequences["input_ids"])
   print(padded_sequences["attention_mask"])
   
   """
   tensor([[101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
           [101, 15526, 65998, 54436, 10127, 51524, 102, 0, 0, 0, 0, 0, 0, 0]])
   tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])
   """
   #上面的数据在末尾有多个0，用于填充成预定长度，无实际意义
   ```

4. ### 特殊 tokens 的意义

   我们在模型的 checkpoints 目录下的配置文件中，经常能看到 eop_token、pad_token、bos_token、eos_token 这些与文本序列处理相关的特殊 `token`，它们代表的意义如下:

   - **bos_token**：序列开始标记，它表示文本序列的起始位置。
   - **eos_token**：**序列结束标记**，它表示文本序列的结束位置。
   - **eop_token**：段落的结束标志，是用于表示段落结束的特殊标记。
   - **pad_token**：填充标记，它用于将文本序列填充到相同长度时使用的特殊 token。

## 二、Pipline管道

| 任务         | 描述                                                     | 模态            | Pipeline                                      |
| ------------ | -------------------------------------------------------- | --------------- | --------------------------------------------- |
| 文本分类     | 为给定的文本序列分配一个标签                             | NLP             | pipeline(task="sentiment-analysis")           |
| 文本生成     | 根据给定的提示生成文本                                   | NLP             | pipeline(task="text-generation")              |
| 命名实体识别 | 为序列里的每个token分配一个标签(人, 组织, 地址等等)      | NLP             | pipeline(task="ner")                          |
| 问答系统     | 通过给定的上下文和问题, 在文本中提取答案                 | NLP             | pipeline(task="question-answering")           |
| 掩盖填充     | 预测出正确的在序列中被掩盖的token                        | NLP             | pipeline(task="fill-mask")                    |
| 文本摘要     | 为文本序列或文档生成总结                                 | NLP             | pipeline(task="summarization")                |
| 文本翻译     | 将文本从一种语言翻译为另一种语言                         | NLP             | pipeline(task="translation")                  |
| 图像分类     | 为图像分配一个标签                                       | Computer vision | pipeline(task="image-classification")         |
| 图像分割     | 为图像中每个独立的像素分配标签(支持语义、全景和实例分割) | Computer vision | pipeline(task="image-segmentation")           |
| 目标检测     | 预测图像中目标对象的边界框和类别                         | Computer vision | pipeline(task="object-detection")             |
| 音频分类     | 给音频文件分配一个标签                                   | Audio           | pipeline(task="audio-classification")         |
| 自动语音识别 | 将音频文件中的语音提取为文本                             | Audio           | pipeline(task="automatic-speech-recognition") |
| 视觉问答     | 给定一个图像和一个问题，正确地回答有关图像的问题         | Multimodal      | pipeline(task="vqa")                          |

代码示例：以下代码是通过 pipeline 函数实现对文本的情绪分类。

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
print(classifier("I've been waiting for a HuggingFace course my whole life."))
# [{'label': 'POSITIVE', 'score': 0.9598049521446228}]
```



## 三、Auto类

1. ### AutoTokenizer 

   AutoTokenizer 是 Hugging Face Transformers 库中用于加载预训练模型对应分词器的高级类。它采用自动工厂模式，可以根据你传入的模型名称或本地路径自动选择正确的分词器，实现分词、编码、批处理、反解码等操作。下面介绍其主要使用方法、常用函数及参数。

    (1) **加载分词器（from_pretrained）**

      - **说明**：

        通过该方法加载预训练分词器，会自动下载（或从缓存加载）对应模型的词表和配置文件，并根据模型类型（`model_type`）选择合适的分词器实现（例如 BERT 使用 `BertTokenizer`，GPT-2 使用 `GPT2Tokenizer`）。

      - **函数原型**：

        ```python
        AutoTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
        ```

      - **常用参数**：

        - `pretrained_model_name_or_path`：字符串或路径，指定预训练模型名称（例如 `"bert-base-chinese"`）或本地模型目录。
        - `use_fast`：布尔值，是否使用 fast 分词器（默认 True）。
        - `cache_dir`：指定缓存目录。
        - `revision`：指定分支、标签或 commit id。
        - 其他特殊 token 参数，如 `bos_token`、`eos_token`、`unk_token`、`pad_token`、`cls_token`、`mask_token`、`additional_special_tokens` 等。

      - **示例代码**：

        ```python
        from transformers import AutoTokenizer
        
        # 通过预训练模型名称加载分词器
        tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
        ```

    (2) **分词（tokenize）**
   
      - **说明**：
   
        将输入文本拆分成单个 token（通常是子词）的列表，不进行数字化转换。
   
      - **函数原型**：
   
        ```python
        tokens = tokenizer.tokenize(text, **kwargs)
        ```
   
      - **示例代码**：
   
        ```python
        text = "今天天气真好"
        tokens = tokenizer.tokenize(text)
        print(tokens)
        # 可能输出: ['今', '天', '天气', '真', '好']
        ```

    (3) **编码（encode）**
   
      - **说明**：
   
        将文本转换成 token ID 列表。该方法会先分词，再将 token 转换为数字表示。
   
      - **函数原型**：
   
        ```python
        token_ids = tokenizer.encode(text, add_special_tokens=True, **kwargs)
        ```
   
      - **常用参数**：
   
        - `add_special_tokens`：是否添加特殊 token（如 [CLS]、[SEP]），默认 True。
   
      - **示例代码**：
   
        ```python
        token_ids = tokenizer.encode("今天天气真好", add_special_tokens=True)
        print(token_ids)
        # 输出类似：[101, 2414, 1184, 3209, 3299, 1045, 102]
        ```
   
    (4) **高级编码（encode_plus）**
   
      - **说明**：
   
        对单个文本或句子对进行编码，返回包含多项输入（例如 `input_ids`、`attention_mask`、`token_type_ids` 等）的字典。
   
      - **函数原型**：
   
        ```python
        encoding = tokenizer.encode_plus(text, text_pair=None, **kwargs)
        ```
   
      - **常用参数**：
   
        - `text_pair`：如果需要处理句子对，可传入第二个句子。
        - `padding`：控制是否填充。常用值有 `True`（自动填充到当前 batch 最大长度）、`"max_length"`（填充到指定的 `max_length`）。
        - `truncation`：是否截断超长文本，设置为 `True` 则自动截断到模型最大长度或 `max_length`。
        - `max_length`：指定最大长度。
        - `return_tensors`：返回张量的类型，如 `"pt"`（PyTorch）、`"tf"`（TensorFlow）、`"np"`（NumPy）。
   
      - **示例代码**：
   
        ```python
        encoding = tokenizer.encode_plus(
            "今天天气真好", 
            add_special_tokens=True,
            padding="max_length",
            truncation=True,
            max_length=10,
            return_tensors="pt"
        )
        print(encoding)
        # 输出包含: input_ids, attention_mask, （以及部分模型返回的 token_type_ids）
        ```

    (5) **分组编码（batch_encode_plus）**
   
      - **说明**：
   
        对一批文本进行编码，自动完成填充、截断等操作，返回的格式与 `encode_plus()` 类似。
   
      - **函数原型**：
   
        ```python
        batch_encoding = tokenizer.batch_encode_plus(text_list, **kwargs)
        ```
   
      - **示例代码**：
   
        ```python
        sentences = ["今天天气真好", "你吃了吗？"]
        batch_encoding = tokenizer.batch_encode_plus(sentences, padding=True, return_tensors="pt")
        print(batch_encoding)
        ```
   
    (6) **解码（decode）**
   
      - **说明**：
   
        将 token ID 列表转换回文本字符串。该方法不仅进行简单的 ID 到 token 映射，还会处理被拆分的词拼接回完整单词。
   
      - **函数原型**：
   
        ```python
        text = tokenizer.decode(token_ids, skip_special_tokens=False, **kwargs)
        ```
   
      - **常用参数**：
   
        - `skip_special_tokens`：是否跳过特殊 token，默认 False。
   
      - **示例代码**：
   
        ```python
        text = tokenizer.decode([101, 2414, 1184, 3209, 3299, 1045, 102], skip_special_tokens=True)
        print(text)
        # 输出：今天天气真好
        ```
   
    (7) **保存词表（save_pretrained）**
   
      - **说明**：
   
        将当前分词器的配置、词表等保存到指定目录，方便后续加载或分享。
   
      - **函数原型**：
   
        ```python
        tokenizer.save_pretrained(save_directory, **kwargs)
        ```
   
      - **示例代码**：
   
        ```python
        tokenizer.save_pretrained("./models/bert-base-chinese/")
        ```
   
        保存后目录下通常会包含：`vocab.txt`（或其他词表文件）、`tokenizer_config.json`、`special_tokens_map.json` 等文件。
   
    (8) **追加词表（add_tokens）**
   
      - **说明**：
   
        用于向现有词表中添加新 token（普通 token 或特殊 token）。
   
      - **函数原型**：
   
        ```python
        num_added_tokens = tokenizer.add_tokens(new_tokens, special_tokens=False)
        ```
   
      - **常用参数**：
   
        - `new_tokens`：新 token 列表。
        - `special_tokens`：如果为 True，则将新 token 视为特殊 token（例如 [ENT_START]、[ENT_END]）。
   
      - **示例代码**：
   
        ```python
        new_tokens = ["[ENT_START]", "[ENT_END]"]
        num_added = tokenizer.add_tokens(new_tokens, special_tokens=True)
        print("添加了", num_added, "个 token")
        # 模型扩展：
        # from transformers import AutoModel
        # model = AutoModel.from_pretrained("bert-base-chinese")
        # model.resize_token_embeddings(len(tokenizer))
        ```
   
        添加新 token 后，如果你同时使用预训练模型，需要调用模型的 `resize_token_embeddings()` 方法调整模型的 embedding 矩阵大小。

2. ### AutoConfig基本类 

    (1) **加载预训练配置（from_pretrained）**

    - **说明**  
      这是加载配置的主要方法，它会根据给定的模型名称或路径，从 Hugging Face Hub（或本地目录）下载并解析配置文件，然后返回相应的配置对象。

    - **函数原型**  
      ```python
      config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
      ```

    - **常用参数**  
      - `pretrained_model_name_or_path`：字符串或路径，指定预训练模型的名称（例如 `"bert-base-cased"`）或本地模型目录。  
      - `cache_dir`：指定缓存目录。  
      - `force_download`：是否强制重新下载配置文件（默认 False）。  
      - `revision`：指定模型的分支、标签或 commit id。  
      - `return_unused_kwargs`：如果为 True，则返回一个元组 `(config, unused_kwargs)`，其中 `unused_kwargs` 为未使用的参数字典。  
      - 其他可以用于更新配置对象的关键字参数，这些参数会覆盖配置文件中已有的值。

    - **示例代码**
      ```python
      from transformers import AutoConfig
      
      # 从 Hugging Face Hub 加载 BERT 的配置文件
      config = AutoConfig.from_pretrained("bert-base-cased")
      print(config)
      ```

    (2) **修改配置**
    
      加载后的配置对象是一个普通的 Python 对象，其属性（如 `hidden_size`、`num_hidden_layers` 等）可以直接访问和修改。例如：
    
      ```python
      # 修改隐藏层大小
      config.hidden_size = 1024
      # 修改 Transformer 层数
      config.num_hidden_layers = 24
      ```

    (3) **保存配置（save_pretrained）**

    - **说明**  
      将当前配置对象保存到指定目录，以便后续加载或分享。

    - **函数原型**  
      ```python
      config.save_pretrained(save_directory, **kwargs)
      ```

    - **常用参数**  
      - `save_directory`：字符串或路径，指定保存配置文件的目录。  
      - 其他参数可参考相关文档。

    - **示例代码**
      ```python
      # 保存配置到本地目录
      config.save_pretrained("./my_model_config/")
      ```
    
3. ### AutoModel基本类 
   
      （1）**加载预训练模型（from_pretrained）**

      - **函数原型**

        ```python
        model = AutoModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
        ```

      - **常用参数：**

        - `pretrained_model_name_or_path`：预训练模型的名称（如 `"bert-base-cased"`、`"gpt2"` 等）或本地存放预训练模型的目录路径。  
        - `model_args`：传递给模型构造函数的额外位置参数。  
        - `config`：可传入一个配置对象（例如通过 `AutoConfig.from_pretrained()` 得到的配置），用于自定义模型参数。  
        - `cache_dir`：指定模型缓存目录。  
        - `force_download`：是否强制重新下载模型文件。  
        - `revision`：可指定模型的分支、标签或 commit id。  
      
   - **示例代码：**
        ```python
        from transformers import AutoModel
          
        # 从 Hugging Face Hub 加载 BERT 模型
        model = AutoModel.from_pretrained("bert-base-cased")
        print(model)
        ```
   
   （2）**AutoModel 常用的函数**
   
      - **forward()**  
        模型前向传播方法，接受诸如 `input_ids`、`attention_mask`、`token_type_ids` 等输入，返回模型输出（例如最后一层隐藏状态）。

      - **chat()**
        
        专为对话场景优化的高级接口，可以接受对话历史列表（含role/content键值对）、自动处理对话上下文拼接，使用于专用Chat版本模型（如Qwen-7B-Chat）
        
   
           ```python
           # 对话式生成（以Qwen-14B-Chat为例）
            messages = [
                {"role": "system", "content": "你是一位Python专家"},
                {"role": "user", "content": "用Python实现斐波那契数列"}
            ]
            response, history = model.chat(
               tokenizer=tokenizer,
                messages=messages,
                temperature=0.3,
                top_p=0.85,
                max_new_tokens=200
           )
           ```
   
   - generate()
   
       通用的底层文本生成方法，提供50+细粒度控制参数，兼容所有生成类模型。
   
       ```python
       # 基础生成模式（以GPT-2为例）
       input_text = """
       <|system|>你是一位Python专家</s>
       <|user|>用Python实现斐波那契数列</s>
       <|assistant|>
       """
       inputs = tokenizer(input_text, return_tensors="pt", add_special_tokens=False)
       outputs = model.generate(
           inputs.input_ids,
           max_new_tokens=200,
           do_sample=True,
           temperature=0.3,
           top_p=0.85,      
           pad_token_id=tokenizer.eos_token_id,
           eos_token_id=tokenizer.convert_tokens_to_ids("<|im_end|>")
       )
       response = tokenizer.decode(outputs[0], skip_special_tokens=False)
       ```
   
       - 常用参数：
            - `max_length`：设置生成文本的最大长度（包括输入文本长度）。  
            - `min_length`：强制生成文本的最小长度，避免过早终止。  
            - `do_sample`：False时默认采用贪心搜索（选择概率最高的token），True时启用温度采样、Top-p/K等随机策略。  
            - `temperature`：调节生成随机性（值越大输出越多样化，值趋近0时接近确定性输出）。  
            - `top_k`：仅从概率最高的top_k个token中采样。  
            - `top_p`：从累计概率达top_k%的最小token集合中采样。
            - `pad_token_id`：指定填充符（指定tokenizer.eos_token_id可以减少模型需要识别的特殊标记数量）。  
            - `eos_token_id`：指定终止符的token ID（如`</s>`），触发生成停止。  
            - `early_stopping`：当所有候选序列均生成终止符时提前结束生成。
            - `repetition_penalty`：惩罚重复出现的token（值>1.0时降低重复概率）。
            - `no_repeat_ngram_size`：阻止连续no_repeat_ngram_size个词重复。
            - `num_beams`：设置束搜索的宽度（值越大生成质量越高，但计算量指数增长）。
            - `length_penalty`：调节生成长度偏好（值>1.0鼓励长文本，值<1.0倾向短文本）。
   
   - **save_pretrained(save_directory, **kwargs)**  
     将模型配置和权重保存到本地目录。  
   
     ```python
     model.save_pretrained("./saved_model/")
     ```
   
   - **resize_token_embeddings(new_num_tokens)**  
     当扩充词表（例如通过分词器的 `add_tokens()`）后，需要调用此方法调整模型的嵌入矩阵大小。  
   
     ```python
     model.resize_token_embeddings(len(tokenizer))
     ```
   
   - **from_config(config)**  
     根据配置对象构造模型，但不加载权重。通常用于自定义配置后构建模型。
   
     此外，AutoModel 作为一个 PyTorch 模型，还支持调用 `eval()`、`train()`、`to()` 等方法。
   
4. ### Automodel专用模型类及其区别

    （1）**AutoModelForCausalLM**

      - **用途**：因果语言建模，主要用于生成任务。  
      - **特点**：  
        - 适用于自回归（unidirectional）文本生成任务，例如 GPT 系列。  
        - 在基础模型上添加了一个线性层作为语言模型 head，用于预测下一个 token。  

    - **示例代码：**
      ```python
      from transformers import AutoModelForCausalLM, AutoTokenizer
      
      tokenizer = AutoTokenizer.from_pretrained("gpt2")
      model = AutoModelForCausalLM.from_pretrained("gpt2")
      input_ids = tokenizer.encode("The meaning of life is", return_tensors="pt")
      output = model.generate(input_ids, max_length=50)
      print(tokenizer.decode(output[0]))
      ```

    （2）**AutoModelForMaskedLM**

    - **用途**：掩码语言建模，用于填空任务。  
    - **特点**：  
      - 适用于双向编码器模型，如 BERT。  
      - 在基础模型上添加了一个预测掩码 token 的 head，用于预测输入中被 mask 的 token。  

    - **示例代码：**
      ```python
      from transformers import AutoModelForMaskedLM, AutoTokenizer
      import torch
      
      tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
      model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
      input_ids = tokenizer.encode("The man worked as a [MASK].", return_tensors="pt")
      outputs = model(input_ids)
      predictions = outputs.logits
      masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]
      predicted_index = predictions[0, masked_index].argmax(-1)
      print(tokenizer.decode(predicted_index))
      ```

    （3）**AutoModelForMultipleChoice**

    - **用途**：多选题任务（例如阅读理解中的答案选择）。  
    - **特点**：  
      - 输入通常为一个包含多个选项的 batch，每个选项对应一个句子或句子对。  
      - 在基础模型上添加了一个分类 head，输出各选项的得分。  

    - **示例代码：**
      ```python
      from transformers import AutoModelForMultipleChoice, AutoTokenizer
      import torch
      
      tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
      model = AutoModelForMultipleChoice.from_pretrained("bert-base-uncased")
      
      # 假设问题与 3 个选项
      question = "What color is the sky?"
      choices = ["Blue", "Green", "Red"]
      
      # 对每个选项构造输入
      inputs = [tokenizer.encode_plus(question, choice, add_special_tokens=True) for choice in choices]
      # 需要构造统一的 tensor 格式： (batch_size, num_choices, sequence_length)
      input_ids = torch.tensor([inp["input_ids"] for inp in inputs]).unsqueeze(0)
      outputs = model(input_ids=input_ids)
      print(outputs.logits)  # 输出每个选项的分数
      ```

    （4）**AutoModelForQuestionAnswering**

    - **用途**：问答任务（如 SQuAD），从一段文本中找出问题的答案。  
    - **特点**：  
      - 在基础模型上添加两个线性层，分别预测答案的起始位置和结束位置。  

    - **示例代码：**
      ```python
      from transformers import AutoModelForQuestionAnswering, AutoTokenizer
      import torch
      
      tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
      model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
      
      question = "What is the repository name?"
      context = "Hugging Face Transformers is a library of pretrained models."
      inputs = tokenizer.encode_plus(question, context, return_tensors="pt")
      outputs = model(**inputs)
      answer_start = torch.argmax(outputs.start_logits)
      answer_end = torch.argmax(outputs.end_logits) + 1
      answer = tokenizer.decode(inputs["input_ids"][0][answer_start:answer_end])
      print(answer)
      ```

    （5）**AutoModelForSequenceClassification**

      - **用途**：序列分类任务（如情感分析）。  
      - **特点**：  
        - 在基础模型上添加一个全连接分类层，通常使用 [CLS] token 的输出作为整个句子的表示，然后映射到类别标签上。  

      - **示例代码：**
        ```python
        from transformers import AutoModelForSequenceClassification, AutoTokenizer
        import torch
        
        tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        
        inputs = tokenizer("I love this movie!", return_tensors="pt")
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=1)
        print(predictions)
        ```

    （6）**AutoModelForTokenClassification**

    - **用途**：逐 token 分类任务（如命名实体识别、POS 标注）。  
    - **特点**：  
      - 在基础模型上添加了一个 token 级别的分类头，每个 token 输出对应的类别标签。  

    - **示例代码：**
      ```python
      from transformers import AutoModelForTokenClassification, AutoTokenizer
      import torch
      
      tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
      model = AutoModelForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
      
      text = "Hugging Face is based in New York."
      inputs = tokenizer(text, return_tensors="pt")
      outputs = model(**inputs)
      predictions = torch.argmax(outputs.logits, dim=2)
      tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
      print(list(zip(tokens, predictions[0].tolist())))
      ```

## 四、大模型调用案例

详情查看[colab笔记本](https://colab.research.google.com/drive/1Mzf6EMWumDEmNKmtOMkgpwKk5-lF0-aN?usp=sharing)
