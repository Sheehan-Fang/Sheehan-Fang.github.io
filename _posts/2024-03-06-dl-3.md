---
title: '长短期记忆神经网络'
date: 2024-03-06
permalink: /posts/2024/03/dl-3/
tags:
  - 深度学习
  - 长短期记忆神经网络
  - 模型架构
---
RNN并不是指特定的某一个模型，而是指一类模型，可以理解为一种结构，凡是使用了这种特定的结构的模型都称为RNN，LSTM就是使用了这种RNN结构的一种模型。

### 一、遗忘门（Forgate Gate）

![](..\images/picture/LSTM/1.webp)

![](..\images/picture/LSTM/2.webp)

- 图中σ代表了sigmoid函数。
- 图中红色圈中的×代表了逐元素相乘。
- sigmoid函数将向量的值映射到0，1之间，这个数值代表了**是否要遗忘该元素**，即当值为0时，代表完全忘记该元素，为1时则代表完全记忆这个元素，如此一来就可以控制接收的 𝐶𝑡−1 的值能否通过。

### 二、输入门（Input Gate）

![](..\images/picture/LSTM/3.webp)

- 向量 𝑖𝑡 的值也都是介于0，1之间，这个向量的作用是控制我们要**更新哪些值**，也就是对于之前学习到的信息哪一些需要更新一下。
- tanh函数会把值映射到-1，1之间。
- C~t表示要在传送带上**增加什么信息**。

### 三、输出门（Output Gate）

![](..\images/picture/LSTM/4.webp)

- LSTM自己控制了**是否更新隐藏状态h**，通过输出门里面的参数矩阵，可以不断地学习到是否更新，而在RNN中是所有的循环单元都输出隐藏状态，因此无法避免梯度消失问题。

### 四、小结

![](..\images/picture/LSTM/5.webp)

LSTM首先计算遗忘门，然后计算输入门和一个待输入的状态 𝐶~𝑡 ，再使用输入门和待输入的状态更新LSTM的记忆细胞C，最后使用更新后的C结合输出门得到最后的隐藏状态h。可以观察到，所有的公式计算最后得到的值都是在0，1之间或者-1，1之间，这就使得在计算门结构的时候可以很方便的进行反向传播，便于更新参数矩阵。
